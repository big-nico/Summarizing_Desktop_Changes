{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlH5deXHdvec",
        "outputId": "9ab192b8-b064-430c-fcf5-e034731e1e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5_b0TzLUn2b",
        "outputId": "cff3b29d-2b2b-4493-9848-d30d5f572bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99MbYEbZV0A2",
        "outputId": "f2c96792-baed-4660-9cdf-12719f0caae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " data\t        data_all_final.zip   full_data.zip   task2   task5\n",
            "'data (1)'      data_smaller.zip     old_data.zip    task3   task6\n",
            " data_alfonso   data.zip\t     task1\t     task4   task7\n"
          ]
        }
      ],
      "source": [
        "!ls gdrive/My\\ Drive/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52JcHik5UoTc"
      },
      "outputs": [],
      "source": [
        "!unzip gdrive/My\\ Drive/data/data_all_final.zip > /dev/null  #for seven tasks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uit0Kk4adxu2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split \n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.datasets import ImageFolder \n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob \n",
        "import random\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "jIZktsgVI70P",
        "outputId": "74da47bc-61e1-4d77-f520-bc79e135458b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 15.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 69.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 74.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 74.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 83.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 78.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 74.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 77.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 78.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 76.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 15.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 78.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 80.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 74.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 60.4 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manuva\u001b[0m (\u001b[33mopenfold\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221207_220655-p0f1a32q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/openfold/jupyter-proj/runs/p0f1a32q\" target=\"_blank\">stilted-water-4</a></strong> to <a href=\"https://wandb.ai/openfold/jupyter-proj\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/openfold/jupyter-proj/runs/p0f1a32q?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f80589e8130>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#init logging to w&b \n",
        "\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "wandb.init(project=\"jupyter-proj\",\n",
        "           config={\n",
        "               \"batch_size\": 100,\n",
        "               \"learning_rate\": 0.01,\n",
        "           })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn6fGoaH7X7e"
      },
      "outputs": [],
      "source": [
        "# move in all of our collected data\n",
        "task1_dataset = glob.glob(os.path.join('./data/task1/', '*.png'))\n",
        "task2_dataset = glob.glob(os.path.join('./data/task2/', '*.png'))\n",
        "task3_dataset = glob.glob(os.path.join('./data/task3/', '*.png'))\n",
        "task4_dataset = glob.glob(os.path.join('./data/task4/', '*.png'))\n",
        "task5_dataset = glob.glob(os.path.join('./data/task5/', '*.png'))\n",
        "task6_dataset = glob.glob(os.path.join('./data/task6/', '*.png'))\n",
        "task7_dataset = glob.glob(os.path.join('./data/task7/', '*.png'))\n",
        "\n",
        "\n",
        "if not os.path.exists(\"./training\"):\n",
        "  os.mkdir(\"./training\")\n",
        "  os.mkdir(\"./training/before\")\n",
        "  os.mkdir(\"./training/after\")\n",
        "if not os.path.exists(\"./validation\"):\n",
        "  os.mkdir(\"./validation\")\n",
        "  os.mkdir(\"./validation/before\")\n",
        "  os.mkdir(\"./validation/after\")\n",
        "if not os.path.exists(\"./test\"): \n",
        "  os.mkdir(\"./test\")\n",
        "  os.mkdir(\"./test/before\")\n",
        "  os.mkdir(\"./test/after\")\n",
        "\n",
        "# this needs to be changed to 7 when we download data correctly\n",
        "tasks = [task1_dataset, task2_dataset, task3_dataset, task4_dataset, task5_dataset, task6_dataset, task7_dataset]\n",
        "# tasks = [task1_dataset, task2_dataset]\n",
        "\n",
        "# now for each task randomly generate numbers for before and after to split\n",
        "for i in range(7):\n",
        "  current_task = tasks[i]\n",
        "  training_size = int(len(current_task) * .8)\n",
        "  test_size = int(len(current_task) * .1)\n",
        "  val_size = int(len(current_task) * .1)\n",
        "\n",
        "  # randomly choose an element\n",
        "  generated_list = [j+1 for j in range(int(len(current_task) / 2))] \n",
        "  \n",
        "  # add elements to our training array\n",
        "  for k in range(int(training_size / 2)):\n",
        "    random_index = random.randint(0,len(generated_list)-1)\n",
        "    random_element = generated_list[random_index]\n",
        "\n",
        "    # add the after and before images corresponding to this number \n",
        "    shutil.copy(f\"./data/task{i+1}/before{random_element}.png\", f\"./training/before/task{i+1}_before_{random_element}.png\")\n",
        "    shutil.copy(f\"./data/task{i+1}/after{random_element}.png\", f\"./training/after/task{i+1}_after_{random_element}.png\")\n",
        "\n",
        "    # remove that this element can be selected again\n",
        "    del generated_list[random_index]\n",
        "\n",
        "\n",
        "  # add elements to our test\n",
        "  for k in range(int(test_size / 2)):\n",
        "\n",
        "    random_index = random.randint(0,len(generated_list)-1)\n",
        "    random_element = generated_list[random_index]\n",
        "\n",
        "    # add the after and before images corresponding to this number \n",
        "    shutil.copy(f\"./data/task{i+1}/before{random_element}.png\", f\"./test/before/task{i+1}_before_{random_element}.png\")\n",
        "    shutil.copy(f\"./data/task{i+1}/after{random_element}.png\", f\"./test/after/task{i+1}_after_{random_element}.png\")\n",
        "\n",
        "    # remove that this element can be selected again\n",
        "    del generated_list[random_index]\n",
        "\n",
        "  \n",
        "  # add elements to our validation\n",
        "  for k in range(int(val_size / 2)):\n",
        "\n",
        "    random_index = random.randint(0,len(generated_list)-1)\n",
        "    random_element = generated_list[random_index]\n",
        "\n",
        "    # add the after and before images corresponding to this number \n",
        "    shutil.copy(f\"./data/task{i+1}/before{random_element}.png\", f\"./validation/before/task{i+1}_before_{random_element}.png\")\n",
        "    shutil.copy(f\"./data/task{i+1}/after{random_element}.png\", f\"./validation/after/task{i+1}_after_{random_element}.png\")\n",
        "\n",
        "    # remove that this element can be selected again\n",
        "    del generated_list[random_index]\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkvNY1-9BKOe"
      },
      "outputs": [],
      "source": [
        "# Image transforms\n",
        "dim = 512\n",
        "def get_train_transform():\n",
        "    return transforms.Compose([\n",
        "    transforms.Resize((dim, dim)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0, 0, 0),(1, 1, 1))\n",
        "    ])\n",
        "\n",
        "def get_val_transform(): \n",
        "    return transforms.Compose([\n",
        "    transforms.Resize((dim,dim)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0, 0, 0),(1, 1, 1))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOlPavmvBAo0"
      },
      "outputs": [],
      "source": [
        "# PyTorch Dataset\n",
        "# Source: https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
        "class CreateDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, imgs, class_to_int, folder_type, transforms = None):\n",
        "        super().__init__()\n",
        "        self.imgs = imgs\n",
        "        self.class_to_int = class_to_int\n",
        "        self.transforms = transforms\n",
        "        self.folder_type = folder_type\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.imgs[idx]\n",
        "        #print(image_name + \"\\n\")\n",
        "        indicators = image_name.split(\"_\")\n",
        "        task_num = indicators[0]\n",
        "        image_num = indicators[2]\n",
        "        \n",
        "        before_dir = f\"./{self.folder_type}/before/\"\n",
        "        before_img = self.transforms(Image.open(before_dir + image_name).convert('RGB'))\n",
        "        after_dir = f\"./{self.folder_type}/after/\"\n",
        "        image_name = task_num + \"_after_\" + image_num\n",
        "        after_img = self.transforms(Image.open(after_dir + image_name).convert('RGB'))\n",
        "\n",
        "        label = self.class_to_int[task_num]\n",
        "        label = torch.tensor(label, dtype = torch.float32)\n",
        "\n",
        "        image_list = [before_img, after_img]\n",
        "\n",
        "        return image_list, label\n",
        "            \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwoS-u1WEo_p"
      },
      "outputs": [],
      "source": [
        "train_dir = \"./training/before/\"\n",
        "train_images = os.listdir(train_dir) \n",
        "\n",
        "val_dir = \"./validation/before/\"\n",
        "val_images = os.listdir(val_dir) \n",
        "\n",
        "test_dir = \"./test/before/\"\n",
        "test_images = os.listdir(test_dir) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgaB9ZOHBNXi"
      },
      "outputs": [],
      "source": [
        "# Create Datasets to be used by Data Loaders\n",
        "class_mapping = {\"task1\" : 0, \"task2\" : 1, \"task3\" : 2, \"task4\": 3, \"task5\": 4, \"task6\": 5, \"task7\": 6}\n",
        "# class_mapping = {\"task1\" : 0, \"task2\" : 1}\n",
        "train_data = CreateDataset(train_images, class_mapping, folder_type = \"training\", transforms = get_train_transform())\n",
        "val_data = CreateDataset(val_images, class_mapping, folder_type = \"validation\", transforms = get_val_transform())\n",
        "test_data = CreateDataset(test_images, class_mapping, folder_type = \"test\", transforms = get_val_transform())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEJNtHB0BN2l",
        "outputId": "f2d083ba-07b9-472a-f10c-f680b0e11e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Data Loaders\n",
        "batch_size = 50\n",
        "num_workers = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "validation_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRrSX0_Cp38Q",
        "outputId": "4f3e9ca2-43c1-4f10-f650-2ef8fd9cd612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num training images: 347\n",
            "Num validation images: 40\n",
            "Num test images: 40\n"
          ]
        }
      ],
      "source": [
        "print('Num training images: ' + str(len(train_loader.dataset)))\n",
        "print('Num validation images: ' + str(len(validation_loader.dataset)))\n",
        "print('Num test images: ' + str(len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwK71hiqJ67p"
      },
      "source": [
        "## Without Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ae2c02d04a354340b56752fa05862b87",
            "bf37a8cbf5494c92bf4690bb9d49447d",
            "4a947d9268fc43e591a269a7f3e6107d",
            "2179797efb21480191be14066c0549e3",
            "eb061f8fba8b400d9aa34d2f7ca50ddf",
            "bfec448513a649f99a761540a3acabf2",
            "32e5e3e6162944f6a957be73c697a6a9",
            "f714956ba21d4b5c93b292d67bf64551",
            "58eec34a30c34c3698116cc2fee74236",
            "723eaba723234b77b73810f7b406679c",
            "9c112eca6416479eab2f019a16ed800e"
          ]
        },
        "id": "kn2oqt1jeLI0",
        "outputId": "93eab4df-84cb-4f74-87ff-d144fc4d25f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae2c02d04a354340b56752fa05862b87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "# Download ResNet50 from online to create our first network for before and the network for after\n",
        "\n",
        "ResNet18 = resnet18(weights=ResNet18_Weights.DEFAULT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYdvx0k9eR_e"
      },
      "outputs": [],
      "source": [
        "# CUT OFF THE LAYERS AT THE END\n",
        "\n",
        "ResNet18.fc = nn.Sequential(\n",
        "   nn.Linear(512, 512)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp0XL8Qwfhhu"
      },
      "outputs": [],
      "source": [
        "# COMBINE BOTH MODELS INTO ONE CLASS\n",
        "class ActionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ActionModel, self).__init__()\n",
        "\n",
        "    # NETWORK A IS OUR FIRST RESNET that encodes the images \n",
        "    self.img_encoder = ResNet18\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 7),\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "\n",
        "  # forward propogation into our model\n",
        "  def forward(self, before_image, after_image):\n",
        "\n",
        "      # run forward prop on the before image\n",
        "      before_encoded = self.img_encoder(before_image)\n",
        "\n",
        "      # run forward prop on the after image\n",
        "      after_encoded = self.img_encoder(after_image)\n",
        "\n",
        "      # combine our encodings\n",
        "      images_combined = torch.cat((before_encoded, after_encoded), 1)\n",
        "\n",
        "      #run final layer of model and return the highest value\n",
        "      return self.fc(images_combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhoqZN8dlxRG"
      },
      "outputs": [],
      "source": [
        "#Define Utility Functions \n",
        "\n",
        "#Accuracy function \n",
        "def accuracy(preds, labels):\n",
        "  _, pred = torch.max(preds, dim=1)\n",
        "  return torch.sum(pred==labels).item()/len(labels)\n",
        "\n",
        "def check_model_accuracy(model, dloader):\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    for batch, (X, y) in enumerate(dloader):\n",
        "        #convert batch into variable that can have gradients applied to them\n",
        "        X[0] = X[0].to(device)\n",
        "        X[1] = X[1].to(device)\n",
        "        y = y.to(device)\n",
        "        #forward pass thru net\n",
        "        preds = model(X[0], X[1])\n",
        "        #check accuracy\n",
        "        acc+=accuracy(preds, y)\n",
        "\n",
        "    return acc/len(dloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxavk9TtoJij"
      },
      "outputs": [],
      "source": [
        "model = ActionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqypb4lKoLga",
        "outputId": "751fdd51-cb53-4c1d-d676-5c787fbbb29e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ActionModel(\n",
              "  (img_encoder): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=7, bias=True)\n",
              "    (5): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "J3BPAQthef_L",
        "outputId": "e0f99854-b7ed-4613-e4a8-ba40de3ad461"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe src=\"https://wandb.ai/openfold/jupyter-proj/runs/3ct45tb7?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7f9e97fec0d0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0  Val Loss:  tensor(1.9470)  Val Accuracy:  0.1\n"
          ]
        }
      ],
      "source": [
        "# TRAIN THE MODEL\n",
        "\n",
        "%%wandb\n",
        "\n",
        "#Freeze initial layers\n",
        "\n",
        "for i, child in enumerate(model.img_encoder.children()):\n",
        "    if i<9:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False \n",
        "    else:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "#init SGD optimizer & CrossEntropyLoss function\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#put on device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "n_epochs = 20\n",
        "step=0\n",
        "\n",
        "#Loop thru epochs \n",
        "for i in range(n_epochs):\n",
        "    val_losses = []\n",
        "    \n",
        "    #loop thru samples\n",
        "    for idx, (X, y) in enumerate(train_loader):\n",
        "        \n",
        "        X[0] = X[0].to(device)\n",
        "        X[1] = X[1].to(device)\n",
        "        mask = torch.unsqueeze(torch.all(torch.eq(X[0], X[1]),axis=1), axis = 1)\n",
        "        # print(X[0].shape)\n",
        "        # print(X[1].shape)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        mask = mask.repeat(1,3,1,1)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        # print(X[0][0].shape)\n",
        "        # print(X[1][0].shape)\n",
        "        # print(mask[0].shape)\n",
        "\n",
        "        result = np.concatenate((X[0][0].cpu().numpy(), X[1][0].cpu().numpy(), mask[0].cpu().numpy()), axis=2)\n",
        "\n",
        "        y = y.to(device)\n",
        "        \n",
        "        #make predictions\n",
        "        preds = model(X[0], X[1])\n",
        "\n",
        "        p_idx = np.argmax(preds.cpu().detach().numpy()[0], axis=0)\n",
        "        gt = str(int(y.cpu().detach()[0].item()))\n",
        "\n",
        "        #calculate loss \n",
        "        loss = loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "\n",
        "        #log the training loss \n",
        "        wandb.log({\"train_loss\": loss})\n",
        "\n",
        "        #backprop\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        #logging \n",
        "        if step%100==0:\n",
        "\n",
        "            #visualize a result from the training\n",
        "            plt.imsave(\"./training_images/epoch_\"+str(i)+\"_step_\"+str(idx)+\"_GT_\"+gt+\"_P_\"+str(p_idx)+\".jpg\", np.moveaxis(result,0,-1))\n",
        "\n",
        "            #calculate accuracy \n",
        "            model.eval()\n",
        "            val_acc=0\n",
        "            val_loss=0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch, (X, y) in enumerate(validation_loader):\n",
        "\n",
        "                    X[0] = X[0].to(device)\n",
        "                    X[1] = X[1].to(device)\n",
        "                    y = y.to(device)\n",
        "                    preds = model(X[0],X[1])\n",
        "                    \n",
        "                    val_loss+=loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "                    val_acc+=accuracy(preds.cpu(), y.cpu())\n",
        "                \n",
        "                #normalize val accuracy and loss\n",
        "                val_acc = val_acc / len(validation_loader)\n",
        "                val_loss = val_loss / len(validation_loader)\n",
        "\n",
        "                wandb.log({\"val_loss\": val_loss})\n",
        "                wandb.log({\"val_acc\": val_acc})\n",
        "                    \n",
        "                #early stopping\n",
        "                if len(val_losses)>5:\n",
        "                    if val_loss >= (sum(val_losses[i-5:i])/5):\n",
        "                        print(\"Stopped at Epoch: \", idx)\n",
        "                        break\n",
        "\n",
        "                val_losses.append(val_loss)\n",
        "        step+=1\n",
        "\n",
        "    if len(val_losses) > 0:\n",
        "      print(\"Epoch: \", i, \" Val Loss: \", val_losses[-1], \" Val Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCtxeGkdejVb",
        "outputId": "ea8ff4ed-39b8-4525-8c98-a46cdce0c64b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.22321428571428573"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "  # TEST THE MODEL\n",
        "\n",
        "#Evaluate the accuracy on the test set.\n",
        "check_model_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTMggfw4l6Xw"
      },
      "outputs": [],
      "source": [
        "#Unfreeze all layers.\n",
        "\n",
        "for i, param in enumerate(model.parameters()):\n",
        "    param.requires_grad = True \n",
        "\n",
        "#Continue Fine Tuning\n",
        "\n",
        "#init SGD optimizer & CrossEntropyLoss function\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#put on device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "n_epochs = 10\n",
        "step=0\n",
        "\n",
        "#Loop thru epochs \n",
        "for i in range(n_epochs):\n",
        "    val_losses = []\n",
        "    \n",
        "    #loop thru samples\n",
        "    for idx, (X, y) in enumerate(train_loader):\n",
        "        \n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        #make predictions\n",
        "        preds = model(X[0], X[1])\n",
        "        \n",
        "        #calculate loss \n",
        "        loss = loss_fn(preds.cpu(), y.cpu())\n",
        "\n",
        "        #backprop\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        #check validation loss & accuracy \n",
        "        if step%100==0:\n",
        "          #calculate accuracy \n",
        "            model.eval()\n",
        "            val_acc=0\n",
        "            val_loss=0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch, (X, y) in enumerate(validation_loader):\n",
        "\n",
        "                    X = X.to(device)\n",
        "                    y = y.to(device)\n",
        "                    preds = model(X[0],X[1])\n",
        "                    \n",
        "                    val_loss+=loss_fn(preds.cpu(), y.cpu())\n",
        "                    val_acc+=accuracy(preds.cpu(), y.cpu())\n",
        "                \n",
        "                #normalize val accuracy and loss\n",
        "                val_acc = val_acc / len(validation_loader)\n",
        "                val_loss = val_loss / len(validation_loader)\n",
        "                    \n",
        "                #early stopping\n",
        "                if len(val_losses)>5:\n",
        "                    if val_loss >= (sum(val_losses[i-5:i])/5):\n",
        "                        print(\"Stopped at Epoch: \", idx)\n",
        "                        break\n",
        "\n",
        "                val_losses.append(val_loss)\n",
        "        step+=1\n",
        "    print(\"Epoch: \", i, \" Val Loss: \", val_losses[-1], \" Val Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVwH7AWZmQ4g"
      },
      "outputs": [],
      "source": [
        "# TEST THE MODEL\n",
        "\n",
        "#Evaluate the accuracy on the test set.\n",
        "check_model_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdR9QAJBJ2ds"
      },
      "source": [
        "## With Mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4517bab309cf49b4bb7fca3a5a65a238",
            "e7cc1227eb58438fa456a7657620796d",
            "a7460d28be4e4a9cbe8ef0f0d60486c9",
            "bf2a61c93d3b465291a8f72ea3178ca1",
            "3e6d3b37bf1b498d8b7421465882f1b5",
            "53acd2127ec74683ad9db76f5161a22a",
            "2756129c6e734981901e4b31ab01b678",
            "5b4ae554c89b4910b16a6816cf8f25c7",
            "a370e1f0e65945f2a15f4a6e4b84f5e9",
            "f49bd46a81544462b904642d3d056473",
            "8ee24a0291be4ae38252da39a25835ac"
          ]
        },
        "id": "Cl_pNCJ7IZT1",
        "outputId": "c07f4ee6-c4eb-4c87-f8e7-33064d5e1894"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4517bab309cf49b4bb7fca3a5a65a238",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "\n",
        "ResNet18 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "ResNet18_mask = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_tTQELwIZWo"
      },
      "outputs": [],
      "source": [
        "# CUT OFF THE LAYERS AT THE END\n",
        "\n",
        "ResNet18.fc = nn.Sequential(\n",
        "   nn.Linear(512, 512)\n",
        ")\n",
        "\n",
        "ResNet18_mask.fc = nn.Sequential(\n",
        "   nn.Linear(512, 512)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P787hXEKIZZZ"
      },
      "outputs": [],
      "source": [
        "# COMBINE BOTH MODELS INTO ONE CLASS\n",
        "class ActionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ActionModel, self).__init__()\n",
        "\n",
        "    # NETWORK A IS OUR FIRST RESNET that encodes the images \n",
        "    self.img_encoder = ResNet18\n",
        "    self.mask_encoder = ResNet18_mask\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(1536,1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 7),\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "\n",
        "  # forward propogation into our model\n",
        "  def forward(self, before_image, after_image, mask_image):\n",
        "\n",
        "      # run forward prop on the before image\n",
        "      before_encoded = self.img_encoder(before_image)\n",
        "\n",
        "      # run forward prop on the after image\n",
        "      after_encoded = self.img_encoder(after_image)\n",
        "\n",
        "      mask_encoded = self.mask_encoder(mask_image)\n",
        "\n",
        "      # combine our encodings\n",
        "      images_combined = torch.cat((before_encoded, after_encoded, mask_encoded), 1)\n",
        "\n",
        "      \n",
        "\n",
        "      #run final layer of model and return the highest value\n",
        "      return self.fc(images_combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDTIoxJ0Ied-"
      },
      "outputs": [],
      "source": [
        "#Define Utility Functions \n",
        "import numpy as np\n",
        "#Accuracy function \n",
        "def accuracy(preds, labels):\n",
        "  _, pred = torch.max(preds, dim=1)\n",
        "  return torch.sum(pred==labels).item()/len(labels)\n",
        "\n",
        "def check_model_accuracy(model, dloader):\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    mistakes = []\n",
        "    incorrect_before = []\n",
        "    incorrect_after = []\n",
        "    incorrect_mask = []\n",
        "    incorrect_labels = []\n",
        "    correct_labels = []\n",
        "\n",
        "    correct_before = []\n",
        "    correct_after = []\n",
        "    correct_mask = []\n",
        "    correct_labels_for_correct = []\n",
        "\n",
        "    for batch, (X, y) in enumerate(dloader):\n",
        "        #convert batch into variable that can have gradients applied to them\n",
        "        X[0] = X[0].to(device)\n",
        "        X[1] = X[1].to(device)\n",
        "\n",
        "        masks = torch.unsqueeze(torch.all(torch.eq(X[0], X[1]),axis=1), axis = 1)\n",
        "\n",
        "        masks_numbers = np.empty(masks.shape)\n",
        "        masks_numbers[np.where(masks.cpu() == True)] = 255\n",
        "        masks_numbers[np.where(masks.cpu() == False)] = 0\n",
        "\n",
        "        masks_numbers_tensor = torch.from_numpy(masks_numbers.astype(np.double)).float()\n",
        "\n",
        "        rgb_masks = masks_numbers_tensor.repeat(1,3,1,1)\n",
        "        rgb_masks = rgb_masks.to(device)\n",
        "        y = y.to(device)\n",
        "        #forward pass thru net\n",
        "        preds = model(X[0], X[1], rgb_masks)\n",
        "\n",
        "        prediction_final = torch.argmax(preds, dim=1).float()\n",
        "        prediction_numpy = prediction_final.cpu().detach().numpy() \n",
        "        target_numpy = y.cpu().numpy()\n",
        "\n",
        "\n",
        "        for i in range(len(prediction_numpy)):\n",
        "          prediction = int(prediction_numpy[i])\n",
        "          target = int(target_numpy[i])\n",
        "\n",
        "          if prediction != target:\n",
        "       \n",
        "            incorrect_before.append(X[0][i].clamp(0,1).cpu().numpy())\n",
        "            incorrect_after.append(X[1][i].clamp(0,1).cpu().numpy())\n",
        "            incorrect_mask.append(rgb_masks[i].clamp(0,1).cpu().numpy() )\n",
        "            incorrect_labels.append(prediction)\n",
        "            correct_labels.append(target)\n",
        "\n",
        "          else:\n",
        "            correct_before.append(X[0][i].clamp(0,1).cpu().numpy())\n",
        "            correct_after.append(X[1][i].clamp(0,1).cpu().numpy())\n",
        "            correct_mask.append(rgb_masks[i].clamp(0,1).cpu().numpy())\n",
        "            correct_labels_for_correct.append(target)\n",
        "\n",
        "\n",
        "        #check accuracy\n",
        "        acc+=accuracy(preds, y)\n",
        "\n",
        "    return acc/len(dloader), correct_before, correct_after, correct_mask, correct_labels_for_correct, incorrect_before, incorrect_after, incorrect_mask, incorrect_labels, correct_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg2_O8qRIegX"
      },
      "outputs": [],
      "source": [
        "model = ActionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bVGVTCzIZcN",
        "outputId": "b0156328-cd23-4d40-f173-7c5cafe300db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ActionModel(\n",
              "  (img_encoder): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (mask_encoder): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=256, out_features=7, bias=True)\n",
              "    (7): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rRxRO7QrIq-v",
        "outputId": "d80ff85c-de1b-41f9-b763-7b100590f6fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe src=\"https://wandb.ai/openfold/jupyter-proj/runs/36c0n293?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7fb9bbe59d60>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9443, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9464, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9474, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9467, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9466, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9448, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9444, grad_fn=<NllLossBackward0>)\n",
            "Epoch:  0  Val Loss:  tensor(1.9458)  Val Accuracy:  0.15\n",
            "tensor(1.9466, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9454, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9443, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9467, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9453, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9458, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9465, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9445, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9452, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9470, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9464, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9457, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9452, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9443, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9458, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9446, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9450, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9445, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9456, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9440, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9471, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9445, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9439, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9438, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9471, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9452, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9459, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9445, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9430, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9445, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9461, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9444, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9445, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9440, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9469, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9450, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9436, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9432, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9439, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9458, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9440, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9458, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9432, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9436, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9443, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9450, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9450, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9435, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9442, grad_fn=<NllLossBackward0>)\n",
            "Epoch:  7  Val Loss:  tensor(1.9444)  Val Accuracy:  0.15\n",
            "tensor(1.9448, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9433, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9434, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9435, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9445, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9425, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9437, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9410, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9427, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9434, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9448, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9438, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9425, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9438, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9445, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9396, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9435, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9423, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9447, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9400, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9430, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9390, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9409, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9422, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9428, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9412, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9425, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9444, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9403, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9421, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9427, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9392, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9416, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9395, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9408, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9397, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9397, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9392, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9362, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9407, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9419, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9412, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9434, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9339, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9379, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9378, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9380, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9401, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9359, grad_fn=<NllLossBackward0>)\n",
            "Epoch:  14  Val Loss:  tensor(1.9398)  Val Accuracy:  0.15\n",
            "tensor(1.9380, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9367, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9355, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9383, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9351, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9345, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9332, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9382, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9393, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9311, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9361, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9339, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9301, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9176, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9305, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9284, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9371, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9305, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9300, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9227, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9062, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9311, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9247, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9246, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9154, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9024, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.8971, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9162, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9208, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.8918, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.8990, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9176, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.8973, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.8919, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.8741, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# TRAIN THE MODEL\n",
        "# %%wandb\n",
        "\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "#Freeze initial layers\n",
        "\n",
        "for i, child in enumerate(model.img_encoder.children()):\n",
        "    if i<9:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False \n",
        "    else:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "#init SGD optimizer & CrossEntropyLoss function\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=.9 )\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#put on device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "n_epochs = 20\n",
        "step=0\n",
        "\n",
        "#Loop thru epochs \n",
        "for i in range(n_epochs):\n",
        "    val_losses = []\n",
        "    \n",
        "    #loop thru samples\n",
        "    for idx, (X, y) in enumerate(train_loader):\n",
        "        \n",
        "        X[0] = X[0].to(device)\n",
        "        X[1] = X[1].to(device)\n",
        "        masks = torch.unsqueeze(torch.all(torch.eq(X[0], X[1]),axis=1), axis = 1)\n",
        "\n",
        "        masks_numbers = np.empty(masks.shape)\n",
        "        masks_numbers[np.where(masks.cpu() == True)] = 255\n",
        "        masks_numbers[np.where(masks.cpu() == False)] = 0\n",
        "\n",
        "        masks_numbers_tensor = torch.from_numpy(masks_numbers.astype(np.double)).float()\n",
        "\n",
        "        rgb_masks = masks_numbers_tensor.repeat(1,3,1,1)\n",
        "        # print(X[0].shape)\n",
        "        # print(X[1].shape)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        mask = masks.repeat(1,3,1,1)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        # print(X[0][0].shape)\n",
        "        # print(X[1][0].shape)\n",
        "        # print(mask[0].shape)\n",
        "\n",
        "        result = np.concatenate((X[0][0].cpu().numpy(), X[1][0].cpu().numpy(), mask[0].cpu().numpy()), axis=2)\n",
        "\n",
        "        y = y.to(device)\n",
        "        \n",
        "        #make predictions\n",
        "        rgb_masks = rgb_masks.to(device)\n",
        "        preds = model(X[0], X[1], rgb_masks)\n",
        "\n",
        "        p_idx = np.argmax(preds.cpu().detach().numpy()[0], axis=0)\n",
        "        gt = str(int(y.cpu().detach()[0].item()))\n",
        "\n",
        "        #calculate loss \n",
        "        loss = loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "        print(loss)\n",
        "\n",
        "        #log the training loss \n",
        "        # wandb.log({\"train_loss\": loss})\n",
        "\n",
        "        #backprop\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        #logging \n",
        "        if step%50==0:\n",
        "\n",
        "            #visualize a result from the training\n",
        "            plt.imsave(\"./training_images/epoch_\"+str(i)+\"_step_\"+str(idx)+\"_GT_\"+gt+\"_P_\"+str(p_idx)+\".jpg\", np.moveaxis(result,0,-1))\n",
        "\n",
        "            #calculate accuracy \n",
        "            model.eval()\n",
        "            val_acc=0\n",
        "            val_loss=0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch, (X, y) in enumerate(validation_loader):\n",
        "\n",
        "                    X[0] = X[0].to(device)\n",
        "                    X[1] = X[1].to(device)\n",
        "          \n",
        "                    masks = torch.unsqueeze(torch.all(torch.eq(X[0], X[1]),axis=1), axis = 1)\n",
        "\n",
        "                    masks_numbers = np.empty(masks.shape)\n",
        "                    masks_numbers[np.where(masks.cpu() == True)] = 255\n",
        "                    masks_numbers[np.where(masks.cpu() == False)] = 0\n",
        "\n",
        "                    masks_numbers_tensor = torch.from_numpy(masks_numbers.astype(np.double)).float()\n",
        "\n",
        "                    rgb_masks = masks_numbers_tensor.repeat(1,3,1,1)\n",
        "                    rgb_masks = rgb_masks.to(device)\n",
        "                    y = y.to(device)\n",
        "                    preds = model(X[0],X[1], rgb_masks)\n",
        "                    \n",
        "                    val_loss+=loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "                    val_acc+=accuracy(preds.cpu(), y.cpu())\n",
        "                \n",
        "                #normalize val accuracy and loss\n",
        "                val_acc = val_acc / len(validation_loader)\n",
        "                val_loss = val_loss / len(validation_loader)\n",
        "\n",
        "                #wandb.log({\"val_loss\": val_loss})\n",
        "                #wandb.log({\"val_acc\": val_acc})\n",
        "                    \n",
        "                #early stopping\n",
        "                if len(val_losses)>5:\n",
        "                    if val_loss >= (sum(val_losses[i-5:i])/5):\n",
        "                        print(\"Stopped at Epoch: \", idx)\n",
        "                        break\n",
        "\n",
        "                val_losses.append(val_loss)\n",
        "        step+=1\n",
        "\n",
        "    if len(val_losses) > 0:\n",
        "      print(\"Epoch: \", i, \" Val Loss: \", val_losses[-1], \" Val Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nglpanI3Is-w"
      },
      "outputs": [],
      "source": [
        "# TEST THE MODEL\n",
        "\n",
        "#Evaluate the accuracy on the test set.\n",
        "check_model_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B290SHgkIZe6"
      },
      "outputs": [],
      "source": [
        "#Unfreeze all layers.\n",
        "\n",
        "for i, param in enumerate(model.parameters()):\n",
        "    param.requires_grad = True \n",
        "\n",
        "#Continue Fine Tuning\n",
        "\n",
        "#init SGD optimizer & CrossEntropyLoss function\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=.9 )\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#put on device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "n_epochs = 20\n",
        "step=0\n",
        "\n",
        "#Loop thru epochs \n",
        "for i in range(n_epochs):\n",
        "    val_losses = []\n",
        "    \n",
        "    #loop thru samples\n",
        "    for idx, (X, y) in enumerate(train_loader):\n",
        "        \n",
        "        X[0] = X[0].to(device)\n",
        "        X[1] = X[1].to(device)\n",
        "        masks = torch.unsqueeze(torch.all(torch.eq(X[0], X[1]),axis=1), axis = 1)\n",
        "\n",
        "        masks_numbers = np.empty(masks.shape)\n",
        "        masks_numbers[np.where(masks.cpu() == True)] = 255\n",
        "        masks_numbers[np.where(masks.cpu() == False)] = 0\n",
        "\n",
        "        masks_numbers_tensor = torch.from_numpy(masks_numbers.astype(np.double)).float()\n",
        "\n",
        "        rgb_masks = masks_numbers_tensor.repeat(1,3,1,1)\n",
        "        # print(X[0].shape)\n",
        "        # print(X[1].shape)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        mask = masks.repeat(1,3,1,1)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        # print(X[0][0].shape)\n",
        "        # print(X[1][0].shape)\n",
        "        # print(mask[0].shape)\n",
        "\n",
        "        result = np.concatenate((X[0][0].cpu().numpy(), X[1][0].cpu().numpy(), mask[0].cpu().numpy()), axis=2)\n",
        "\n",
        "        y = y.to(device)\n",
        "        \n",
        "        #make predictions\n",
        "        rgb_masks = rgb_masks.to(device)\n",
        "        preds = model(X[0], X[1], rgb_masks)\n",
        "\n",
        "        p_idx = np.argmax(preds.cpu().detach().numpy()[0], axis=0)\n",
        "        gt = str(int(y.cpu().detach()[0].item()))\n",
        "\n",
        "        #calculate loss \n",
        "        loss = loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "        print(loss)\n",
        "\n",
        "        #log the training loss \n",
        "        # wandb.log({\"train_loss\": loss})\n",
        "\n",
        "        #backprop\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        #logging \n",
        "        if step%50==0:\n",
        "\n",
        "            #visualize a result from the training\n",
        "            plt.imsave(\"./training_images/epoch_\"+str(i)+\"_step_\"+str(idx)+\"_GT_\"+gt+\"_P_\"+str(p_idx)+\".jpg\", np.moveaxis(result,0,-1))\n",
        "\n",
        "            #calculate accuracy \n",
        "            model.eval()\n",
        "            val_acc=0\n",
        "            val_loss=0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch, (X, y) in enumerate(validation_loader):\n",
        "\n",
        "                    X[0] = X[0].to(device)\n",
        "                    X[1] = X[1].to(device)\n",
        "          \n",
        "                    masks = torch.unsqueeze(torch.all(torch.eq(X[0], X[1]),axis=1), axis = 1)\n",
        "\n",
        "                    masks_numbers = np.empty(masks.shape)\n",
        "                    masks_numbers[np.where(masks.cpu() == True)] = 255\n",
        "                    masks_numbers[np.where(masks.cpu() == False)] = 0\n",
        "\n",
        "                    masks_numbers_tensor = torch.from_numpy(masks_numbers.astype(np.double)).float()\n",
        "\n",
        "                    rgb_masks = masks_numbers_tensor.repeat(1,3,1,1)\n",
        "                    rgb_masks = rgb_masks.to(device)\n",
        "                    y = y.to(device)\n",
        "                    preds = model(X[0],X[1], rgb_masks)\n",
        "                    \n",
        "                    val_loss+=loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "                    val_acc+=accuracy(preds.cpu(), y.cpu())\n",
        "                \n",
        "                #normalize val accuracy and loss\n",
        "                val_acc = val_acc / len(validation_loader)\n",
        "                val_loss = val_loss / len(validation_loader)\n",
        "\n",
        "                #wandb.log({\"val_loss\": val_loss})\n",
        "                #wandb.log({\"val_acc\": val_acc})\n",
        "                    \n",
        "                #early stopping\n",
        "                if len(val_losses)>5:\n",
        "                    if val_loss >= (sum(val_losses[i-5:i])/5):\n",
        "                        print(\"Stopped at Epoch: \", idx)\n",
        "                        break\n",
        "\n",
        "                val_losses.append(val_loss)\n",
        "        step+=1\n",
        "\n",
        "    if len(val_losses) > 0:\n",
        "      print(\"Epoch: \", i, \" Val Loss: \", val_losses[-1], \" Val Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHOu0y10f-MT"
      },
      "outputs": [],
      "source": [
        "# loading the model and evaluating it is easier than running the above\n",
        "torch.save(model.state_dict(), \"./model_learning_rate_1e-3.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNfewmufgAg9"
      },
      "outputs": [],
      "source": [
        "newModel = ActionModel()\n",
        "newModel.load_state_dict(torch.load(\"./model_learning_rate_1e-3.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGd5LX2hgAj4"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afUW5yUVgCss"
      },
      "outputs": [],
      "source": [
        "#put on device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "newModel.to(device)\n",
        "\n",
        "accuracy , correct_before, correct_after, correct_mask, correct_labels_for_correct, incorrect_before, incorrect_after, incorrect_mask, incorrect_labels, correct_labels = check_model_accuracy(newModel, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwWH_cwMf-PF"
      },
      "outputs": [],
      "source": [
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k9-SflXgF5v"
      },
      "outputs": [],
      "source": [
        "\n",
        "# let us plot the first 5 sample mistakes\n",
        "for i in range(5):\n",
        "\n",
        "  mistake_img_before = incorrect_before[i]\n",
        "  mistake_img_after = incorrect_after[i]\n",
        "  mistake_img_mask =  incorrect_mask[i]\n",
        "\n",
        "\n",
        "  result = np.concatenate((mistake_img_before, mistake_img_after, mistake_img_mask), axis=2)\n",
        "  result = np.moveaxis(result, 0, -1)\n",
        "\n",
        "  plt.imsave(\"./sample_mistakes/predicted_\" +  str(incorrect_labels[i]) + \"_correct_\"+ str(correct_labels[i]) + \".jpg\", result)\n",
        "  # plt.show()\n",
        "  print(\"PREDICTED LABEL: \" + str(incorrect_labels[i]))\n",
        "  print(\"CORRECT LABEL: \" + str(correct_labels[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENtfKEmmgICN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# let us plot the first 5 sample mistakes\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  correct_img_before = correct_before[i]\n",
        "  correct_img_after = correct_after[i]\n",
        "  correct_img_mask =  correct_mask[i]\n",
        "\n",
        "  result = np.concatenate((correct_img_before, correct_img_after, correct_img_mask), axis=2)\n",
        "  result = np.moveaxis(result, 0, -1)\n",
        "  print(result.shape)\n",
        "  plt.imsave(\"./sample_correct/correct_\"+ str(correct_labels_for_correct[i]) + \".jpg\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKk_jy7mgF8g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX0ddcxaudEw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfO6AhQbVV3g"
      },
      "source": [
        "## Representation Difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71oneBr5VZ02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "de7066bef2bc474d905e248d4cf21f36",
            "7ca8cdda26324b15a1f567a5662d2232",
            "1aee11c3a54542529bb1b4c21da6caff",
            "b198e8ea5c1240b9837e7bdb02d00656",
            "04f4efc0d643421b8d70fa0ccae70601",
            "b9117fed75d649d68892566843db3a74",
            "5473ae0a85d543a8909514b92c094f28",
            "09d3da48e95843f287b86b840a701630",
            "bb41904569544df8875f753292d78ac0",
            "bc6b791e0de543b9ba269298bba49715",
            "22730b70ae894c608e6886a480dc3262"
          ]
        },
        "outputId": "4abfa704-6b82-4eda-f6ed-e64b1aded74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de7066bef2bc474d905e248d4cf21f36"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "\n",
        "ResNet18 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "ResNet18_mask = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zelJPdMbVaPh"
      },
      "outputs": [],
      "source": [
        "# CUT OFF THE LAYERS AT THE END\n",
        "\n",
        "ResNet18.fc = nn.Sequential(\n",
        "   nn.Linear(512, 512)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGEVegbrVaSD"
      },
      "outputs": [],
      "source": [
        "# COMBINE BOTH MODELS INTO ONE CLASS\n",
        "class ActionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ActionModel, self).__init__()\n",
        "\n",
        "    # NETWORK A IS OUR FIRST RESNET that encodes the images \n",
        "    self.img_encoder = ResNet18\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(1536,1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 7),\n",
        "        nn.Softmax()\n",
        "    )\n",
        "\n",
        "\n",
        "  # forward propogation into our model\n",
        "  def forward(self, before_image, after_image):\n",
        "\n",
        "      # run forward prop on the before image\n",
        "      before_encoded = self.img_encoder(before_image)\n",
        "\n",
        "      # run forward prop on the after image\n",
        "      after_encoded = self.img_encoder(after_image)\n",
        "\n",
        "      difference = after_encoded - before_encoded\n",
        "\n",
        "      # combine our encodings\n",
        "      images_combined = torch.cat((before_encoded, difference, after_encoded), 1)\n",
        "\n",
        "      #run final layer of model and return the highest value\n",
        "      return self.fc(images_combined)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSGsFHVgWtgN"
      },
      "outputs": [],
      "source": [
        "#Define Utility Functions \n",
        "\n",
        "#Accuracy function \n",
        "def accuracy(preds, labels):\n",
        "  _, pred = torch.max(preds, dim=1)\n",
        "  return torch.sum(pred==labels).item()/len(labels)\n",
        "\n",
        "def check_model_accuracy(model, dloader):\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    for batch, (X, y) in enumerate(dloader):\n",
        "        #convert batch into variable that can have gradients applied to them\n",
        "        X[0] = X[0].to(device)\n",
        "        X[1] = X[1].to(device)\n",
        "        y = y.to(device)\n",
        "        #forward pass thru net\n",
        "        preds = model(X[0], X[1])\n",
        "        #check accuracy\n",
        "        acc+=accuracy(preds, y)\n",
        "\n",
        "    return acc/len(dloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLrE539oXKpu"
      },
      "outputs": [],
      "source": [
        "model = ActionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO8siT5sW6eS",
        "outputId": "60fdb2c4-cf01-440a-8bb9-4ad338817eb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ActionModel(\n",
              "  (img_encoder): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=256, out_features=7, bias=True)\n",
              "    (7): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "o50ji55dVhtc",
        "outputId": "f2095c83-732b-4e24-90d4-f9773eedcfb0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7f805685ed60>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/openfold/jupyter-proj/runs/p0f1a32q?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0  Val Loss:  tensor(1.9463)  Val Accuracy:  0.125\n",
            "Epoch:  14  Val Loss:  tensor(1.9462)  Val Accuracy:  0.125\n"
          ]
        }
      ],
      "source": [
        "# TRAIN THE MODEL\n",
        "\n",
        "%%wandb\n",
        "\n",
        "#Freeze initial layers\n",
        "\n",
        "for i, child in enumerate(model.img_encoder.children()):\n",
        "    if i<9:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False \n",
        "    else:\n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "#init SGD optimizer & CrossEntropyLoss function\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#put on device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "n_epochs = 20\n",
        "step=0\n",
        "\n",
        "#Loop thru epochs \n",
        "for i in range(n_epochs):\n",
        "    val_losses = []\n",
        "    \n",
        "    #loop thru samples\n",
        "    for idx, (X, y) in enumerate(train_loader):\n",
        "        \n",
        "        X[0] = X[0].to(device)\n",
        "        X[1] = X[1].to(device)\n",
        "        mask = torch.unsqueeze(torch.all(torch.eq(X[0], X[1]),axis=1), axis = 1)\n",
        "        # print(X[0].shape)\n",
        "        # print(X[1].shape)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        mask = mask.repeat(1,3,1,1)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        # print(X[0][0].shape)\n",
        "        # print(X[1][0].shape)\n",
        "        # print(mask[0].shape)\n",
        "\n",
        "        result = np.concatenate((X[0][0].cpu().numpy(), X[1][0].cpu().numpy(), mask[0].cpu().numpy()), axis=2)\n",
        "\n",
        "        y = y.to(device)\n",
        "        \n",
        "        #make predictions\n",
        "        preds = model(X[0], X[1])\n",
        "\n",
        "        p_idx = np.argmax(preds.cpu().detach().numpy()[0], axis=0)\n",
        "        gt = str(int(y.cpu().detach()[0].item()))\n",
        "\n",
        "        #calculate loss \n",
        "        loss = loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "\n",
        "        #log the training loss \n",
        "        wandb.log({\"train_loss\": loss})\n",
        "\n",
        "        #backprop\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        #logging \n",
        "        if step%100==0:\n",
        "\n",
        "            #visualize a result from the training\n",
        "            plt.imsave(\"./training_images/epoch_\"+str(i)+\"_step_\"+str(idx)+\"_GT_\"+gt+\"_P_\"+str(p_idx)+\".jpg\", np.moveaxis(result,0,-1))\n",
        "\n",
        "            #calculate accuracy \n",
        "            model.eval()\n",
        "            val_acc=0\n",
        "            val_loss=0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch, (X, y) in enumerate(validation_loader):\n",
        "\n",
        "                    X[0] = X[0].to(device)\n",
        "                    X[1] = X[1].to(device)\n",
        "                    y = y.to(device)\n",
        "                    preds = model(X[0],X[1])\n",
        "                    \n",
        "                    val_loss+=loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "                    val_acc+=accuracy(preds.cpu(), y.cpu())\n",
        "                \n",
        "                #normalize val accuracy and loss\n",
        "                val_acc = val_acc / len(validation_loader)\n",
        "                val_loss = val_loss / len(validation_loader)\n",
        "\n",
        "                wandb.log({\"val_loss\": val_loss})\n",
        "                wandb.log({\"val_acc\": val_acc})\n",
        "                    \n",
        "                #early stopping\n",
        "                if len(val_losses)>5:\n",
        "                    if val_loss >= (sum(val_losses[i-5:i])/5):\n",
        "                        print(\"Stopped at Epoch: \", idx)\n",
        "                        break\n",
        "\n",
        "                val_losses.append(val_loss)\n",
        "        step+=1\n",
        "\n",
        "    if len(val_losses) > 0:\n",
        "      print(\"Epoch: \", i, \" Val Loss: \", val_losses[-1], \" Val Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW3gVc57XOaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4160487-ef7e-4818-8435-0f7a5a074563"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.175"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "  # TEST THE MODEL\n",
        "\n",
        "#Evaluate the accuracy on the test set.\n",
        "check_model_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsdUInUBXZ1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "510c4994-f68b-4a34-8e3c-d3f96e7b11dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7f805685e550>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/openfold/jupyter-proj/runs/p0f1a32q?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0  Val Loss:  tensor(1.9460)  Val Accuracy:  0.175\n"
          ]
        }
      ],
      "source": [
        "%%wandb\n",
        "\n",
        "#Unfreeze all layers.\n",
        "\n",
        "for i, param in enumerate(model.parameters()):\n",
        "    param.requires_grad = True \n",
        "\n",
        "#Continue Fine Tuning\n",
        "\n",
        "#init SGD optimizer & CrossEntropyLoss function\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#put on device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "n_epochs = 10\n",
        "step=0\n",
        "\n",
        "#Loop thru epochs \n",
        "for i in range(n_epochs):\n",
        "    val_losses = []\n",
        "    \n",
        "    #loop thru samples\n",
        "    for idx, (X, y) in enumerate(train_loader):\n",
        "        \n",
        "        X[0] = X[0].to(device)\n",
        "        X[1] = X[1].to(device)\n",
        "        mask = torch.unsqueeze(torch.all(torch.eq(X[0], X[1]),axis=1), axis = 1)\n",
        "        # print(X[0].shape)\n",
        "        # print(X[1].shape)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        mask = mask.repeat(1,3,1,1)\n",
        "        # print(mask.shape)\n",
        "\n",
        "        # print(X[0][0].shape)\n",
        "        # print(X[1][0].shape)\n",
        "        # print(mask[0].shape)\n",
        "\n",
        "        result = np.concatenate((X[0][0].cpu().numpy(), X[1][0].cpu().numpy(), mask[0].cpu().numpy()), axis=2)\n",
        "\n",
        "        y = y.to(device)\n",
        "        \n",
        "        #make predictions\n",
        "        preds = model(X[0], X[1])\n",
        "\n",
        "        p_idx = np.argmax(preds.cpu().detach().numpy()[0], axis=0)\n",
        "        gt = str(int(y.cpu().detach()[0].item()))\n",
        "\n",
        "        #calculate loss \n",
        "        loss = loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "\n",
        "        #log the training loss \n",
        "        wandb.log({\"train_loss\": loss})\n",
        "\n",
        "        #backprop\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        #logging \n",
        "        if step%100==0:\n",
        "\n",
        "            #visualize a result from the training\n",
        "            plt.imsave(\"./training_images/epoch_\"+str(i)+\"_step_\"+str(idx)+\"_GT_\"+gt+\"_P_\"+str(p_idx)+\".jpg\", np.moveaxis(result,0,-1))\n",
        "\n",
        "            #calculate accuracy \n",
        "            model.eval()\n",
        "            val_acc=0\n",
        "            val_loss=0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch, (X, y) in enumerate(validation_loader):\n",
        "\n",
        "                    X[0] = X[0].to(device)\n",
        "                    X[1] = X[1].to(device)\n",
        "                    y = y.to(device)\n",
        "                    preds = model(X[0],X[1])\n",
        "                    \n",
        "                    val_loss+=loss_fn(preds.cpu(), y.cpu().to(torch.long))\n",
        "                    val_acc+=accuracy(preds.cpu(), y.cpu())\n",
        "                \n",
        "                #normalize val accuracy and loss\n",
        "                val_acc = val_acc / len(validation_loader)\n",
        "                val_loss = val_loss / len(validation_loader)\n",
        "\n",
        "                wandb.log({\"val_loss\": val_loss})\n",
        "                wandb.log({\"val_acc\": val_acc})\n",
        "                    \n",
        "                #early stopping\n",
        "                if len(val_losses)>5:\n",
        "                    if val_loss >= (sum(val_losses[i-5:i])/5):\n",
        "                        print(\"Stopped at Epoch: \", idx)\n",
        "                        break\n",
        "\n",
        "                val_losses.append(val_loss)\n",
        "        step+=1\n",
        "\n",
        "    if len(val_losses) > 0:\n",
        "      print(\"Epoch: \", i, \" Val Loss: \", val_losses[-1], \" Val Accuracy: \", val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AMBNl5fXfEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a71a91-30e3-4af5-8e5a-a7e42579c6d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.175"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# TEST THE MODEL\n",
        "\n",
        "#Evaluate the accuracy on the test set.\n",
        "check_model_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5xMPV1PXfHL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6NpgrhTVhv2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hza_80D2VaUn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2179797efb21480191be14066c0549e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_723eaba723234b77b73810f7b406679c",
            "placeholder": "​",
            "style": "IPY_MODEL_9c112eca6416479eab2f019a16ed800e",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 100MB/s]"
          }
        },
        "2756129c6e734981901e4b31ab01b678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32e5e3e6162944f6a957be73c697a6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e6d3b37bf1b498d8b7421465882f1b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4517bab309cf49b4bb7fca3a5a65a238": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7cc1227eb58438fa456a7657620796d",
              "IPY_MODEL_a7460d28be4e4a9cbe8ef0f0d60486c9",
              "IPY_MODEL_bf2a61c93d3b465291a8f72ea3178ca1"
            ],
            "layout": "IPY_MODEL_3e6d3b37bf1b498d8b7421465882f1b5"
          }
        },
        "4a947d9268fc43e591a269a7f3e6107d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f714956ba21d4b5c93b292d67bf64551",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58eec34a30c34c3698116cc2fee74236",
            "value": 46830571
          }
        },
        "53acd2127ec74683ad9db76f5161a22a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58eec34a30c34c3698116cc2fee74236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b4ae554c89b4910b16a6816cf8f25c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "723eaba723234b77b73810f7b406679c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee24a0291be4ae38252da39a25835ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c112eca6416479eab2f019a16ed800e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a370e1f0e65945f2a15f4a6e4b84f5e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7460d28be4e4a9cbe8ef0f0d60486c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b4ae554c89b4910b16a6816cf8f25c7",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a370e1f0e65945f2a15f4a6e4b84f5e9",
            "value": 46830571
          }
        },
        "ae2c02d04a354340b56752fa05862b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf37a8cbf5494c92bf4690bb9d49447d",
              "IPY_MODEL_4a947d9268fc43e591a269a7f3e6107d",
              "IPY_MODEL_2179797efb21480191be14066c0549e3"
            ],
            "layout": "IPY_MODEL_eb061f8fba8b400d9aa34d2f7ca50ddf"
          }
        },
        "bf2a61c93d3b465291a8f72ea3178ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f49bd46a81544462b904642d3d056473",
            "placeholder": "​",
            "style": "IPY_MODEL_8ee24a0291be4ae38252da39a25835ac",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 63.3MB/s]"
          }
        },
        "bf37a8cbf5494c92bf4690bb9d49447d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfec448513a649f99a761540a3acabf2",
            "placeholder": "​",
            "style": "IPY_MODEL_32e5e3e6162944f6a957be73c697a6a9",
            "value": "100%"
          }
        },
        "bfec448513a649f99a761540a3acabf2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7cc1227eb58438fa456a7657620796d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53acd2127ec74683ad9db76f5161a22a",
            "placeholder": "​",
            "style": "IPY_MODEL_2756129c6e734981901e4b31ab01b678",
            "value": "100%"
          }
        },
        "eb061f8fba8b400d9aa34d2f7ca50ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49bd46a81544462b904642d3d056473": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f714956ba21d4b5c93b292d67bf64551": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de7066bef2bc474d905e248d4cf21f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ca8cdda26324b15a1f567a5662d2232",
              "IPY_MODEL_1aee11c3a54542529bb1b4c21da6caff",
              "IPY_MODEL_b198e8ea5c1240b9837e7bdb02d00656"
            ],
            "layout": "IPY_MODEL_04f4efc0d643421b8d70fa0ccae70601"
          }
        },
        "7ca8cdda26324b15a1f567a5662d2232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9117fed75d649d68892566843db3a74",
            "placeholder": "​",
            "style": "IPY_MODEL_5473ae0a85d543a8909514b92c094f28",
            "value": "100%"
          }
        },
        "1aee11c3a54542529bb1b4c21da6caff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09d3da48e95843f287b86b840a701630",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb41904569544df8875f753292d78ac0",
            "value": 46830571
          }
        },
        "b198e8ea5c1240b9837e7bdb02d00656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc6b791e0de543b9ba269298bba49715",
            "placeholder": "​",
            "style": "IPY_MODEL_22730b70ae894c608e6886a480dc3262",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 65.8MB/s]"
          }
        },
        "04f4efc0d643421b8d70fa0ccae70601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9117fed75d649d68892566843db3a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5473ae0a85d543a8909514b92c094f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09d3da48e95843f287b86b840a701630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb41904569544df8875f753292d78ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc6b791e0de543b9ba269298bba49715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22730b70ae894c608e6886a480dc3262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}